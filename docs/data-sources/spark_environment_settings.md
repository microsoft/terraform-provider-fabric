---
# generated by https://github.com/hashicorp/terraform-plugin-docs
page_title: "fabric_spark_environment_settings Data Source - terraform-provider-fabric"
subcategory: ""
description: |-
  The Spark Environment Settings data-source allows you to retrieve details about a Fabric Spark Environment Settings https://learn.microsoft.com/fabric/data-engineering/environment-manage-compute.
  -> This data-source supports Service Principal authentication.
  ~> This data-source is in preview. To access it, you must explicitly enable the preview mode in the provider level configuration.
---

# fabric_spark_environment_settings (Data Source)

The Spark Environment Settings data-source allows you to retrieve details about a Fabric [Spark Environment Settings](https://learn.microsoft.com/fabric/data-engineering/environment-manage-compute).

-> This data-source supports Service Principal authentication.

~> This data-source is in **preview**. To access it, you must explicitly enable the `preview` mode in the provider level configuration.

## Example Usage

```terraform
data "fabric_spark_environment_settings" "example" {
  workspace_id       = "00000000-0000-0000-0000-000000000000"
  environment_id     = "11111111-1111-1111-1111-111111111111"
  publication_status = "Published"
}
```

<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `environment_id` (String) The Environment ID.
- `publication_status` (String) Publication status. Value must be one of : `Published`, `Staging`.
- `workspace_id` (String) The Workspace ID.

### Optional

- `timeouts` (Attributes) (see [below for nested schema](#nestedatt--timeouts))

### Read-Only

- `driver_cores` (Number) Publication status. Value must be one of : `4`, `8`, `16`, `32`, `64`.
- `driver_memory` (String) Spark driver memory. Value must be one of : `28g`, `56g`, `112g`, `224g`, `400g`.
- `dynamic_executor_allocation` (Attributes) Dynamic Executor Allocation properties. (see [below for nested schema](#nestedatt--dynamic_executor_allocation))
- `executor_cores` (Number) Spark executor core. Value must be one of : `4`, `8`, `16`, `32`, `64`.
- `executor_memory` (String) Spark executor memory. Value must be one of : `28g`, `56g`, `112g`, `224g`, `400g`.
- `id` (String) The Spark Environment Settings ID.
- `pool` (Attributes) Environment pool. (see [below for nested schema](#nestedatt--pool))
- `runtime_version` (String) [Runtime](https://review.learn.microsoft.com/fabric/data-engineering/runtime) version. Value must be one of : `1.1`, `1.2`, `1.3`.
- `spark_properties` (Map of String) A map of key/value pairs of Spark properties.

<a id="nestedatt--timeouts"></a>

### Nested Schema for `timeouts`

Optional:

- `read` (String) A string that can be [parsed as a duration](https://pkg.go.dev/time#ParseDuration) consisting of numbers and unit suffixes, such as "30s" or "2h45m". Valid time units are "s" (seconds), "m" (minutes), "h" (hours).

<a id="nestedatt--dynamic_executor_allocation"></a>

### Nested Schema for `dynamic_executor_allocation`

Read-Only:

- `enabled` (Boolean) The status of the dynamic executor allocation: `false` - Disabled, `true` - Enabled.
- `max_executors` (Number) The maximum executors.
- `min_executors` (Number) The minimum executors.

<a id="nestedatt--pool"></a>

### Nested Schema for `pool`

Read-Only:

- `id` (String) The Pool ID.
- `name` (String) The Pool name. `Starter Pool` means using the starting pool.
- `type` (String) The Pool type. Value must be one of : `Capacity`, `Workspace`.
