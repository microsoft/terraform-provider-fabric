---
# generated by https://github.com/hashicorp/terraform-plugin-docs
page_title: "fabric_spark_job_definition Resource - terraform-provider-fabric"
subcategory: ""
description: |-
  The Spark Job Definition resource allows you to manage a Fabric Spark Job Definition https://learn.microsoft.com/fabric/data-engineering/spark-job-definition.
  -> This resource supports Service Principal authentication.
---

# fabric_spark_job_definition (Resource)

The Spark Job Definition resource allows you to manage a Fabric [Spark Job Definition](https://learn.microsoft.com/fabric/data-engineering/spark-job-definition).

-> This resource supports Service Principal authentication.

## Example Usage

```terraform
# Example 1 - Item without definition
resource "fabric_spark_job_definition" "example" {
  display_name = "example1"
  workspace_id = "00000000-0000-0000-0000-000000000000"
  folder_id    = "11111111-1111-1111-1111-111111111111"
}

# Example 2 - Item with definition bootstrapping only
resource "fabric_spark_job_definition" "example_definition_bootstrap" {
  display_name              = "example2"
  description               = "example with definition bootstrapping"
  workspace_id              = "00000000-0000-0000-0000-000000000000"
  definition_update_enabled = false
  format                    = "SparkJobDefinitionV1"
  definition = {
    "SparkJobDefinitionV1.json" = {
      source = "${local.path}/SparkJobDefinitionV1.json.tmpl"
    }
  }
}

# Example 3 - Item with definition update when source or tokens changed
resource "fabric_spark_job_definition" "example_definition_update" {
  display_name = "example3"
  description  = "example with definition update when source or tokens changed"
  workspace_id = "00000000-0000-0000-0000-000000000000"
  format       = "SparkJobDefinitionV1"
  definition = {
    "SparkJobDefinitionV1.json" = {
      source = "${local.path}/SparkJobDefinitionV1.json.tmpl"
      tokens = {
        "DefaultLakehouseID"     = "11111111-1111-1111-1111-111111111111"
        "AdditionalLakehouseIDs" = "\"22222222-2222-2222-2222-222222222222\",\"33333333-3333-3333-3333-333333333333\""
      }
    }
  }
}
```

<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `display_name` (String) The Spark Job Definition display name.
- `workspace_id` (String) The Workspace ID.

### Optional

- `definition` (Attributes Map) Definition parts. Read more about [Spark Job Definition definition part paths](https://learn.microsoft.com/rest/api/fabric/articles/item-management/definitions/spark-job-definition). Accepted path keys: **SparkJobDefinitionV1** format: `SparkJobDefinitionV1.json` (see [below for nested schema](#nestedatt--definition))
- `definition_update_enabled` (Boolean) Update definition on change of source content. Default: `true`.
- `description` (String) The Spark Job Definition description.
- `folder_id` (String) The Folder ID.
- `format` (String) The Spark Job Definition format. Possible values: `SparkJobDefinitionV1`
- `timeouts` (Attributes) (see [below for nested schema](#nestedatt--timeouts))

### Read-Only

- `id` (String) The Spark Job Definition ID.
- `properties` (Attributes) The Spark Job Definition properties. (see [below for nested schema](#nestedatt--properties))

<a id="nestedatt--definition"></a>

### Nested Schema for `definition`

Required:

- `source` (String) Path to the file with source of the definition part.

The source content may include placeholders for token substitution. Use the dot with the token name `{{ .TokenName }}`.

Optional:

- `tokens` (Map of String) A map of key/value pairs of tokens substitutes in the source.

Read-Only:

- `source_content_sha256` (String) SHA256 of source's content of definition part.

<a id="nestedatt--timeouts"></a>

### Nested Schema for `timeouts`

Optional:

- `create` (String) A string that can be [parsed as a duration](https://pkg.go.dev/time#ParseDuration) consisting of numbers and unit suffixes, such as "30s" or "2h45m". Valid time units are "s" (seconds), "m" (minutes), "h" (hours).
- `delete` (String) A string that can be [parsed as a duration](https://pkg.go.dev/time#ParseDuration) consisting of numbers and unit suffixes, such as "30s" or "2h45m". Valid time units are "s" (seconds), "m" (minutes), "h" (hours). Setting a timeout for a Delete operation is only applicable if changes are saved into state before the destroy operation occurs.
- `read` (String) A string that can be [parsed as a duration](https://pkg.go.dev/time#ParseDuration) consisting of numbers and unit suffixes, such as "30s" or "2h45m". Valid time units are "s" (seconds), "m" (minutes), "h" (hours). Read operations occur during any refresh or planning operation when refresh is enabled.
- `update` (String) A string that can be [parsed as a duration](https://pkg.go.dev/time#ParseDuration) consisting of numbers and unit suffixes, such as "30s" or "2h45m". Valid time units are "s" (seconds), "m" (minutes), "h" (hours).

<a id="nestedatt--properties"></a>

### Nested Schema for `properties`

Read-Only:

- `onelake_root_path` (String) OneLake path to the Spark Job Definition root directory.

## Import

Import is supported using the following syntax:

The [`terraform import` command](https://developer.hashicorp.com/terraform/cli/commands/import) can be used, for example:

```shell
# terraform import fabric_spark_job_definition.example "<WorkspaceID>/<SparkJobDefinitionID>"
terraform import fabric_spark_job_definition.example "00000000-0000-0000-0000-000000000000/11111111-1111-1111-1111-111111111111"
```
