---
# generated by https://github.com/hashicorp/terraform-plugin-docs
page_title: "fabric_spark_workspace_settings Resource - terraform-provider-fabric"
subcategory: ""
description: |-
  The Spark Workspace Settings resource allows you to manage a Fabric Spark Workspace Settings https://learn.microsoft.com/fabric/data-engineering/workspace-admin-settings.
  -> This resource supports Service Principal authentication.
  ~> This resource is in preview. To access it, you must explicitly enable the preview mode in the provider level configuration.
---

# fabric_spark_workspace_settings (Resource)

The Spark Workspace Settings resource allows you to manage a Fabric [Spark Workspace Settings](https://learn.microsoft.com/fabric/data-engineering/workspace-admin-settings).

-> This resource supports Service Principal authentication.

~> This resource is in **preview**. To access it, you must explicitly enable the `preview` mode in the provider level configuration.

## Example Usage

```terraform
# Example 1
resource "fabric_spark_workspace_settings" "example" {
  workspace_id = "00000000-0000-0000-0000-000000000000"

  automatic_log = {
    /*
   your settings here
   */
  }

  high_concurrency = {
    /*
   your settings here
   */
  }

  environment = {
    /*
   your settings here
   */
  }

  job = {
    /*
   your settings here
   */
  }

  pool = {
    /*
   your settings here
   */
  }
}

# Example 2
resource "fabric_spark_workspace_settings" "example2" {
  workspace_id = "00000000-0000-0000-0000-000000000000"

  automatic_log = {
    enabled = true
  }

  high_concurrency = {
    notebook_interactive_run_enabled = false
  }

  environment = {
    name            = "MyExampleEnvironment"
    runtime_version = "1.3"
  }

  pool = {
    default_pool = {
      name = "MyExampleCustomPool"
      type = "Workspace"
    }
    starter_pool = {
      max_executors  = 3
      max_node_count = 1
    }
    customize_compute_enabled = true
  }
}
```

<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `workspace_id` (String) <i style="color:red;font-weight: bold">(ForceNew)</i> The Workspace ID.

### Optional

- `automatic_log` (Attributes) Automatic Log properties. (see [below for nested schema](#nestedatt--automatic_log))
- `environment` (Attributes) Environment properties. (see [below for nested schema](#nestedatt--environment))
- `high_concurrency` (Attributes) High Concurrency properties. (see [below for nested schema](#nestedatt--high_concurrency))
- `job` (Attributes) Job properties. (see [below for nested schema](#nestedatt--job))
- `pool` (Attributes) Pool properties. (see [below for nested schema](#nestedatt--pool))
- `timeouts` (Attributes) (see [below for nested schema](#nestedatt--timeouts))

### Read-Only

- `id` (String) The Spark Workspace Settings ID.

<a id="nestedatt--automatic_log"></a>

### Nested Schema for `automatic_log`

Optional:

- `enabled` (Boolean) The status of the automatic log: `false` - Disabled, `true` - Enabled.

<a id="nestedatt--environment"></a>

### Nested Schema for `environment`

Optional:

- `name` (String) The name of the environment.
- `runtime_version` (String) [Runtime](https://review.learn.microsoft.com/fabric/data-engineering/runtime) version. Value must be one of : `1.1`, `1.2`, `1.3`.

<a id="nestedatt--high_concurrency"></a>

### Nested Schema for `high_concurrency`

Optional:

- `notebook_interactive_run_enabled` (Boolean) The status of the high concurrency for notebook interactive run: `false` - Disabled, `true` - Enabled.
- `notebook_pipeline_run_enabled` (Boolean) The status of the high concurrency for notebook pipeline run: `false` - Disabled, `true` - Enabled.

<a id="nestedatt--job"></a>

### Nested Schema for `job`

Optional:

- `conservative_job_admission_enabled` (Boolean) Reserve maximum cores for active Spark jobs. When this setting is enabled, your Fabric capacity reserves the maximum number of cores needed for active Spark jobs, ensuring job reliability by making sure that cores are available if a job scales up. When this setting is disabled, jobs are started based on the minimum number of cores needed, letting more jobs run at the same time. `false` - Disabled, `true` - Enabled.
- `session_timeout_in_minutes` (Number) Time to terminate inactive Spark sessions. The maximum is 14 days (20160 minutes). Value must be at most 20160.

<a id="nestedatt--pool"></a>

### Nested Schema for `pool`

Optional:

- `customize_compute_enabled` (Boolean) Customize compute configurations for items. `false` - Disabled, `true` - Enabled.
- `default_pool` (Attributes) Default pool for workspace. (see [below for nested schema](#nestedatt--pool--default_pool))
- `starter_pool` (Attributes) Starter pool for workspace. For more information about configuring starter pool, see [configuring starter pool](https://review.learn.microsoft.com/fabric/data-engineering/configure-starter-pools). (see [below for nested schema](#nestedatt--pool--starter_pool))

<a id="nestedatt--pool--default_pool"></a>

### Nested Schema for `pool.default_pool`

Optional:

- `id` (String) The Pool ID. `00000000-0000-0000-0000-000000000000` means use the starter pool. Ensure that if an attribute is set, these are not set: "[<.name,<.type]".
- `name` (String) The Pool name. `Starter Pool` means use the starting pool. Ensure that if an attribute is set, also these are set: "[<.type]". Ensure that if an attribute is set, these are not set: "[<.id]".
- `type` (String) The Pool type. Value must be one of : `Capacity`, `Workspace`. Ensure that if an attribute is set, also these are set: "[<.name]". Ensure that if an attribute is set, these are not set: "[<.id]".

<a id="nestedatt--pool--starter_pool"></a>

### Nested Schema for `pool.starter_pool`

Optional:

- `max_executors` (Number) The maximum executors count.
- `max_node_count` (Number) The maximum node count.

<a id="nestedatt--timeouts"></a>

### Nested Schema for `timeouts`

Optional:

- `create` (String) A string that can be [parsed as a duration](https://pkg.go.dev/time#ParseDuration) consisting of numbers and unit suffixes, such as "30s" or "2h45m". Valid time units are "s" (seconds), "m" (minutes), "h" (hours).
- `delete` (String) A string that can be [parsed as a duration](https://pkg.go.dev/time#ParseDuration) consisting of numbers and unit suffixes, such as "30s" or "2h45m". Valid time units are "s" (seconds), "m" (minutes), "h" (hours). Setting a timeout for a Delete operation is only applicable if changes are saved into state before the destroy operation occurs.
- `read` (String) A string that can be [parsed as a duration](https://pkg.go.dev/time#ParseDuration) consisting of numbers and unit suffixes, such as "30s" or "2h45m". Valid time units are "s" (seconds), "m" (minutes), "h" (hours). Read operations occur during any refresh or planning operation when refresh is enabled.
- `update` (String) A string that can be [parsed as a duration](https://pkg.go.dev/time#ParseDuration) consisting of numbers and unit suffixes, such as "30s" or "2h45m". Valid time units are "s" (seconds), "m" (minutes), "h" (hours).
